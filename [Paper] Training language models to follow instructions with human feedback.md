# Training language models to follow instructions with human feedback

[Paper Link](https://arxiv.org/abs/2203.02155)

## Table of Contents
1. [Abstract](#Abstract)
2. [Introduction](#Introduction)
3. 

## Abstract
![image](https://github.com/user-attachments/assets/53f79251-f5db-42c6-9eb6-7c9a6eb4532e)

*언어 모델을 크게 키우는 것이 반드시 사용자의 의도를 더 잘 따르는 것은 아니다.* 대형 언어 모델은 거짓되거나, 유해하거나, 사용자에게 도움이 되지 않는 출력을 생성할 수 있다. 즉, 이러한 모델들은 사용자와 '정렬'되어 있지 않다. 본 논문은 <mark>**인간 피드백을 통한 미세조정으로 다양한 작업에서 언어 모델을 사용자 의도와 정렬하는 방법**</mark>을 보여준다.
<br><br>

### 연구 과정:  
1. 레이블러가 작성한 프롬프트와 OpenAI API 제출 프롬프트로 시작하여 레이블러 시연 데이터셋을 수집
2. 이를 사용하여 지도 학습으로 GPT-3를 fine tuning
3. 모델 출력의 순위 데이터셋을 수집하여 인간 피드백 기반 강화학습으로 추가 fine tuning
<br><br>

이렇게 만들어진 모델은 'InstructGPT'이다. 프롬프트 분포에 대한 인간 평가에서, 1.3B 파라미터의 InstructGPT 모델 출력이 175B GPT-3(파라미터 100배 많음)의 출력보다 더 선호되었다. 또한 InstructGPT 모델은 공개 NLP 데이터셋의 성능 저하를 최소화하면서도 진실성이 향상되고 유해 출력이 감소하였다. InstructGPT가 여전히 단순한 실수를 하지만, 이 연구 결과는 인간 피드백을 통한 미세조정이 언어 모델을 인간의 의도와 정렬하는 데 있어 유망한 방향임을 보여주는 것이다.
<br><br>
레이블러(데이터 라벨링 작업자) 시연 데이터: 모델이 어떻게 응답해야하는지 직접 시연하여 작성한 예시 데이터
<br><br>

### 인간 피드백

- 지도 학습 단계에서 레이블러들이 만든 시연 데이터로 모델 학습
- 강화 학습 단계에서 모델이 생성한 여러 답변들의 순위를 매기는 방식으로 피드백
<br><br>

### 전체 과정 요약

1. 레이블러가 프롬프트에 대한 이상적인 응답 작성
2. 이 데이터로 GPT-3 fine tuning
3. fine tuned 모델의 여러 출력들에 대해 인간이 순위 매김
4. 이 순위 데이터를 사용해 강화학습으로 다시 fine tuning
<br><br>

## Introduction

대규모 언어 모델들(LMs)은 작업에 대한 몇 가지 예시를 입력으로 제공받으면 다양한 자연어 처리(NLP) 작업을 수행하도록 "프롬프팅" 될 수 있다. 하지만 이러한 모델들은 종종 *사실을 날조하거나, 편향되거나 유해한 텍스트를 생성하거나, 단순히 사용자의 지시를 따르지 않는 등의 의도치 않은 행동*을 보인다(Bender 외., 2021; Bommasani 외., 2021; Kenton 외., 2021; Weidinger 외., 2021; Tamkin 외., 2021; Gehman 외., 2020). 이는 최근 많은 대규모 LM에 사용되는 <mark>**언어 모델링 목표(인터넷에서 웹 페이지의 다음 토큰 예측)가 “사용자의 지침을 유용하고 안전하게 따르기”라는 목표와 다르기 때문**</mark>이다(Radford 외., 2019; Brown 외., 2020; Fedus 외., 2021; Rae 외., 2021; Thoppilan 외., 2022). 따라서 우리는 이러한 언어 모델링 목적이 '잘못 정렬되어 있다'고 말한다. 이러한 의도치 않은 행동들을 방지하는 것은 수백 개의 애플리케이션에서 배포되고 사용되는 언어 모델들에게 특히 중요한 것이다.
<br><br>
우리는 *사용자의 의도에 따라 행동하도록 언어 모델을 훈련*시켜 모델 정렬에서 진전을 이루었다(Leike 외., 2018). 이는 지시사항을 따르는 것과 같은 명시적 의도와, 진실성 유지, 편향되지 않음, 유해하지 않음과 같은 암묵적 의도 모두를 포함한다. Askell 외.(2021)의 용어를 사용하면, 우리는 언어 모델이 '도움이 되고'(사용자가 과제를 해결하도록 도와야 함), '정직하며'(정보를 조작하거나 사용자를 오도해서는 안 됨), '무해한'(사람이나 환경에 물리적, 심리적, 사회적 해를 끼치지 않아야 함) 것을 원한다. 이러한 기준의 평가에 대해서는 3.6절에서 자세히 설명한다.
<br><br>
우리는 언어 모델 정렬을 위한 미세조정 접근법에 초점을 맞추었다. 구체적으로, 우리는 인간 피드백을 통한 강화학습(RLHF; Christiano 외., 2017; Stiennon 외., 2020)을 사용하여 GPT-3가 광범위한 written instructions를 따르도록 미세조정했다. 이 기법은 ***인간의 선호도를 보상 신호로 사용하여 모델을 fine tuning***한다. 먼저 40명의 계약업체로 구성된 팀을 고용하여 선별 테스트의 성과를 바탕으로 데이터에 라벨을 붙인다(자세한 내용은 섹션 3.4 및 부록 B.1 참조). 그런 다음 OpenAI API³에 제출된 (주로 영어) 프롬프트와 일부 레이블러가 작성한 프롬프트에 대해 원하는 출력 동작을 보여주는 인간 작성 데모 데이터셋을 수집하여 지도학습 기준 모델을 훈련시킨다. 다음으로, 더 큰 API 프롬프트 세트에 대한 우리 모델들의 출력을 비교한 인간 레이블 데이터셋을 수집하고, 이 데이터셋에서 보상 모델(RM)을 훈련시켜 레이블러가 선호할 모델 출력을 예측한다. 마지막으로, 이 *RM을 보상 함수로 사용하여 PPO 알고리즘(Schulman 외., 2017)으로 지도학습 기준 모델을 미세조정해 이 보상을 최대화*한다. 이 절차는 GPT-3의 행동을 특정 그룹(주로 우리의 레이블러와 연구원들)의 명시된 선호도에 맞추는 것이며, "인간 가치"라는 더 광범위한 개념에 맞추는 것이 아니다; 이에 대해서는 5.2절에서 더 논의한다. 우리는 이렇게 만들어진 모델들을 *InstructGPT*라 부른다.
우리는 주로 테스트 세트(훈련 데이터에 포함되지 않은 고객들의 프롬프트로 구성)에 대한 모델 출력의 품질을 레이블러가 평가하는 방식으로 모델을 평가한다. 또한 다양한 공개 NLP 데이터셋에 대해 자동 평가도 수행한다.
![image](https://github.com/user-attachments/assets/e760e6de-8126-4ef0-acc8-8e745a57c4ac)

### 1. RM(Reward Model, 보상 모델):
- 인간이 모델 출력들의 품질을 비교하여 순위를 매긴 데이터로 학습된 모델
- 새로운 모델 출력이 들어오면 그것이 얼마나 "좋은" 출력인지 점수를 매기는 역할
<br><br>

### 2. PPO(Proximal Policy Optimization) 알고리즘:
- 강화학습에서 사용되는 알고리즘
- 모델이 RM에서 높은 점수를 받을 수 있는 출력을 더 자주 생성하도록 학습
<br><br>

### 전체 과정:
- 기존의 지도학습 모델이 출력을 생성한다
- RM이 이 출력의 품질을 평가하여 점수를 매긴다
- PPO 알고리즘이 이 점수를 바탕으로 모델을 조금씩 수정한다
- 이 과정을 반복하여 점점 더 좋은 출력을 생성하는 모델로 발전시킨다
<br><br>

⇒ 인간이 좋다고 평가할 만한 출력을 더 많이 생성하도록 모델을 지속적으로 개선하는 과정


레이블러들은 InstructGPT의 출력을 GPT-3의 출력보다 크게 선호한다. 테스트 세트에서, **1.3B 파라미터를 가진 InstructGPT 모델의 출력이 100배 이상 많은 파라미터를 가진 175B GPT-3의 출력보다 더 선호**되었다. 이 모델들은 같은 구조를 가지며, 단지 InstructGPT가 우리의 인간 데이터로 미세조정되었다는 점만이 다르다. 이러한 결과는 GPT-3에 few-shot 프롬프트를 추가하여 지시사항을 더 잘 따르게 만들었을 때도 유지되었다. InstructGPT 모델들은 레이블러들의 평가에 따르면 더 적절한 출력을 생성하고, 지시사항의 명시적 제약을 더 안정적으로 따른다.
<br><br>
InstructGPT 모델들은 GPT-3보다 **진실성이 향상**되었다. TruthfulQA 벤치마크에서 InstructGPT는 GPT-3보다 약 두 배 더 자주 진실되고 유익한 답변을 생성했다. 이러한 결과는 GPT-3에 대해 적대적으로 선택되지 않은 질문들의 하위 집합에서도 똑같이 강력했다. API 프롬프트 분포에서 "closed-domain" 작업(입력에 없는 정보를 출력에 포함하면 안 되는 요약 및 closed-domain QA 등)에서, InstructGPT 모델들은 GPT-3의 절반 수준으로 입력에 없는 정보를 만들어냈다(환각 비율이 각각 21% vs. 41%).
<br><br>
InstructGPT는 GPT-3보다 **유해성은 약간 개선되었으나, 편향성은 개선되지 않았다**. 유해성을 측정하기 위해, 우리는 RealToxicityPrompts 데이터셋(Gehman 외., 2020)을 사용하여 자동 및 인간 평가를 수행했다. InstructGPT 모델들은 존중하는 태도를 요구받았을 때 GPT-3보다 약 25% 적은 유해한 출력을 생성했다. 하지만 InstructGPT는 Winogender(Rudinger 외., 2018)와 CrowSPairs(Nangia 외., 2020) 데이터셋에서 GPT-3보다 유의미한 개선을 보이지 않았다.
<br><br>

성능 저하 관련
<br><br>
우리는 RLHF 미세조정 절차를 수정하여 공개 NLP 데이터셋에서의 성능 저하를 최소화할 수 있다. RLHF 미세조정 중에, 우리는 특정 공개 NLP 데이터셋, 특히 SQuAD (Rajpurkar 외., 2018), DROP (Dua 외., 2019), HellaSwag (Zellers 외., 2019), WMT 2015 프랑스어-영어 번역 (Bojar 외., 2015)에서 GPT-3와 비교했을 때 성능 저하를 관찰했다. 이는 "정렬 세금"의 예시인데, 우리의 정렬 절차가 신경 쓸 만한 특정 작업들의 성능 저하를 대가로 이루어지기 때문이다. 우리는 **PPO 업데이트와 사전학습 분포의 로그 우도를 증가시키는 업데이트(PPO-ptx)를 혼합함으로써, 레이블러 선호도 점수를 손상시키지 않으면서 이러한 데이터셋들의 성능 저하를 크게 줄일 수 있다.**
<br><br>
모델 일반화
<br><br>
우리의 모델은 학습 데이터를 생성하지 않은 "held-out" 레이블러들의 선호도에도 일반화된다. 모델의 일반화를 테스트하기 위해, 우리는 held-out 레이블러들과 예비 실험을 수행했고, 그들이 InstructGPT 출력을 GPT-3 출력보다 우리의 학습 레이블러들과 비슷한 비율로 선호한다는 것을 발견했다. 하지만 이러한 모델들이 더 넓은 사용자 그룹에서 어떻게 수행되는지, 그리고 **인간들이 원하는 행동에 대해 의견이 불일치하는 입력에서 어떻게 수행되는지 연구하기 위해서는 더 많은 작업이 필요**하다.
<br><br>
공개 NLP 데이터셋
<br><br>
공개 NLP 데이터셋은 우리의 언어 모델이 어떻게 사용되는지를 반영하지 않는다. 우리는 인간 선호도 데이터로 미세조정된 GPT-3(즉, InstructGPT)를 두 가지 다른 공개 NLP 작업 모음으로 미세조정된 GPT-3와 비교했다: FLAN (Wei 외., 2021)과 T0(특히 T0++ 변형) (Sanh 외., 2021). 이 데이터셋들은 각 작업에 대한 자연어 지시사항과 함께 다양한 NLP 작업들로 구성되어 있다. 우리의 API 프롬프트 분포에서, FLAN과 T0 모델들은 우리의 SFT 기준선보다 약간 더 나쁜 성능을 보였고, 레이블러들은 이러한 모델들보다 InstructGPT를 크게 선호했다(InstructGPT는 73.4 ±2% 승률을 보인 반면, T0와 FLAN 버전은 각각 26.8 ±2%와 29.8 ±2%를 기록했다).

RLHF 외 지시사항 일반화
InstructGPT 모델들은 RLHF 미세조정 분포 외의 지시사항에도 유망한 일반화를 보여준다. 우리는 InstructGPT의 능력을 정성적으로 조사했고, 미세조정 분포에서 매우 드문 지시사항임에도 불구하고 코드 요약, 코드 관련 질문 답변, 때로는 다른 언어의 지시사항을 따르는 것이 가능하다는 것을 발견했다. 반면에 GPT-3는 이러한 작업들을 수행할 수는 있지만 더 신중한 프롬프팅이 필요하고, 보통 이러한 영역에서 지시사항을 따르지 않는다. 이 결과는 우리의 모델들이 "지시사항 따르기"의 개념을 일반화할 수 있다는 점에서 흥미롭다. 그들은 매우 적은 직접적인 감독 신호를 받는 작업에서도 어느 정도의 정렬을 유지한다.

한계점과 향후 과제
InstructGPT도 여전히 단순한 실수를 한다. 예를 들어, InstructGPT는 여전히 지시사항을 따르지 못하거나, 사실을 지어내거나, 간단한 질문에 긴 모호한 답변을 하거나, 잘못된 전제를 가진 지시사항을 감지하지 못할 수 있다.
전반적으로, 우리의 결과는 인간 선호도를 사용한 대형 언어 모델의 미세조정이 광범위한 작업에서 그들의 행동을 크게 개선한다는 것을 보여주지만, 그들의 안전성과 신뢰성을 개선하기 위해서는 많은 작업이 남아있다.

논문 구조
이 논문의 나머지 부분은 다음과 같이 구성된다: 먼저 2절에서 관련 연구를 상세히 설명하고, 3절에서는 우리의 방법과 실험 세부사항을 다룬다. 여기에는 고수준 방법론(3.1), 작업 및 데이터셋 세부사항(3.3과 3.2), 인간 데이터 수집(3.4), 모델 학습 방법(3.5), 평가 절차(3.6)가 포함된다. 그런 다음 4절에서 결과를 제시하는데, 이는 API 프롬프트 분포에 대한 결과(4.1), 공개 NLP 데이터셋에 대한 결과(4.2), 정성적 결과(4.3)의 세 부분으로 나뉜다. 마지막으로 5절에서 우리의 연구에 대한 광범위한 논의를 진행하며, 여기에는 정렬 연구에 대한 시사점(5.1), 우리가 무엇에 정렬하는지(5.2), 한계점(5.3), 열린 질문(5.4), 이 연구의 더 넓은 영향(5.5)이 포함된다.
