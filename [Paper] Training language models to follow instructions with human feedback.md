# Training language models to follow instructions with human feedback

[Paper Link](https://arxiv.org/abs/2203.02155)

## Table of Contents
1. [Abstract](#Abstract)
2. [Introduction](#usage)
3. [Conclusion](#conclusion)

## Abstract
![image](https://github.com/user-attachments/assets/53f79251-f5db-42c6-9eb6-7c9a6eb4532e)

*언어 모델을 크게 키우는 것이 반드시 사용자의 의도를 더 잘 따르는 것은 아니다.* 대형 언어 모델은 거짓되거나, 유해하거나, 사용자에게 도움이 되지 않는 출력을 생성할 수 있다. 즉, 이러한 모델들은 사용자와 '정렬'되어 있지 않다. 본 논문은 <mark>**인간 피드백을 통한 미세조정으로 다양한 작업에서 언어 모델을 사용자 의도와 정렬하는 방법**</mark>을 보여준다. 

### 연구 과정: 

1. 레이블러가 작성한 프롬프트와 OpenAI API 제출 프롬프트로 시작하여 레이블러 시연 데이터셋을 수집
2. 이를 사용하여 지도 학습으로 GPT-3를 fine tuning
3. 모델 출력의 순위 데이터셋을 수집하여 인간 피드백 기반 강화학습으로 추가 fine tuning

이렇게 만들어진 모델은 'InstructGPT'이다. 프롬프트 분포에 대한 인간 평가에서, 1.3B 파라미터의 InstructGPT 모델 출력이 175B GPT-3(파라미터 100배 많음)의 출력보다 더 선호되었다. 또한 InstructGPT 모델은 공개 NLP 데이터셋의 성능 저하를 최소화하면서도 진실성이 향상되고 유해 출력이 감소하였다. InstructGPT가 여전히 단순한 실수를 하지만, 이 연구 결과는 인간 피드백을 통한 미세조정이 언어 모델을 인간의 의도와 정렬하는 데 있어 유망한 방향임을 보여주는 것이다. 

레이블러(데이터 라벨링 작업자) 시연 데이터: 모델이 어떻게 응답해야하는지 직접 시연하여 작성한 예시 데이터

### 인간 피드백

- 지도 학습 단계에서 레이블러들이 만든 시연 데이터로 모델 학습
- 강화 학습 단계에서 모델이 생성한 여러 답변들의 순위를 매기는 방식으로 피드백

### 전체 과정 요약

1. 레이블러가 프롬프트에 대한 이상적인 응답 작성
2. 이 데이터로 GPT-3 fine tuning
3. fine tuned 모델의 여러 출력들에 대해 인간이 순위 매김
4. 이 순위 데이터를 사용해 강화학습으로 다시 fine tuning

## Introduction 

대규모 언어 모델들(LMs)은 작업에 대한 몇 가지 예시를 입력으로 제공받으면 다양한 자연어 처리(NLP) 작업을 수행하도록 "프롬프팅" 될 수 있다. 하지만 이러한 모델들은 종종 사실을 날조하거나, 편향되거나 유해한 텍스트를 생성하거나, 단순히 사용자의 지시를 따르지 않는 등의 의도치 않은 행동을 보인다(Bender 외., 2021; Bommasani 외., 2021; Kenton 외., 2021; Weidinger 외., 2021; Tamkin 외., 2021; Gehman 외., 2020). 이는 최근 많은 대규모 LM에 사용되는 언어 모델링 목표(인터넷에서 웹 페이지의 다음 토큰 예측)가 “사용자의 지침을 유용하고 안전하게 따르기”라는 목표와 다르기 때문이다(Radford 외., 2019; Brown 외., 2020; Fedus 외., 2021; Rae 외., 2021; Thoppilan 외., 2022). 따라서 우리는 이러한 언어 모델링 목적이 '잘못 정렬되어 있다'고 말한다. 이러한 의도치 않은 행동들을 방지하는 것은 수백 개의 애플리케이션에서 배포되고 사용되는 언어 모델들에게 특히 중요한 것이다.
